{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_6Xe-2WYWjZ",
        "outputId": "5361204c-cf18-4b56-c5be-22da71e91077"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import ast\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from tqdm import tqdm\n",
        "\n",
        "class GradientSteering:\n",
        "    def __init__(self, model_name=\"google/flan-t5-small\"):\n",
        "        print(f\"Loading {model_name}...\")\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        self.baseline_cache = {}\n",
        "        self.baseline_text_cache = {}\n",
        "\n",
        "    def load_data(self, jsonl_content):\n",
        "        data = []\n",
        "        for line in jsonl_content.strip().split('\\n'):\n",
        "            if not line.strip(): continue\n",
        "            try:\n",
        "                row = json.loads(line)\n",
        "                if isinstance(row['classes'], str): classes = ast.literal_eval(row['classes'])\n",
        "                else: classes = row['classes']\n",
        "\n",
        "                target_str = classes[row['answer_index']].strip(\" .\")\n",
        "                trap_str = classes[1 - row['answer_index']].strip(\" .\")\n",
        "\n",
        "                tgt_id = self.tokenizer(\" \" + target_str).input_ids[0]\n",
        "                trap_id = self.tokenizer(\" \" + trap_str).input_ids[0]\n",
        "\n",
        "                data.append({\n",
        "                    \"prompt\": row['prompt'],\n",
        "                    \"target_str\": target_str,\n",
        "                    \"target_id\": tgt_id,\n",
        "                    \"trap_id\": trap_id\n",
        "                })\n",
        "            except: continue\n",
        "        return data\n",
        "\n",
        "    def mine_gradient_vector(self, train_data, layer_idx):\n",
        "        print(f\"\\n--- Mining Gradients (Layer {layer_idx}) ---\")\n",
        "        gradients = []\n",
        "\n",
        "        for item in tqdm(train_data[:100], desc=\"Mining\"):\n",
        "\n",
        "            input_ids = self.tokenizer(item['prompt'], return_tensors=\"pt\").input_ids.to(self.device)\n",
        "            dec_ids = self.tokenizer(\"<pad>\", return_tensors=\"pt\", add_special_tokens=False).input_ids.to(self.device)\n",
        "\n",
        "            # Forward pass- enable grad\n",
        "            with torch.enable_grad():\n",
        "                outputs = self.model(input_ids=input_ids, decoder_input_ids=dec_ids, output_hidden_states=True)\n",
        "                hidden = outputs.decoder_hidden_states[layer_idx]\n",
        "                hidden.retain_grad()\n",
        "\n",
        "                logits = outputs.logits\n",
        "\n",
        "                # Contrastive objective: log(P_trap) - log(P_target)\n",
        "                target_logit = logits[0, -1, item['target_id']]\n",
        "                trap_logit = logits[0, -1, item['trap_id']]\n",
        "                loss = trap_logit - target_logit\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                loss.backward(retain_graph=True)\n",
        "\n",
        "                # Extract gradient (invert as loss is trap-target)\n",
        "                grad_vec = -hidden.grad.detach() # [1, 1, hidden]\n",
        "                gradients.append(grad_vec.squeeze())\n",
        "\n",
        "        # average, normalize\n",
        "        global_vec = torch.stack(gradients).mean(dim=0)\n",
        "        global_vec = global_vec / global_vec.norm()\n",
        "\n",
        "        print(f\"Gradient Vector Extracted. Norm: {global_vec.norm().item():.4f}\")\n",
        "        return global_vec\n",
        "\n",
        "    def test(self, test_data, vector, layer_idx, coeff, num_samples=-1):\n",
        "        print(f\"\\n--- Testing Layer {layer_idx} | Strength {coeff} ---\")\n",
        "\n",
        "        # steering\n",
        "        def hook(module, input, output):\n",
        "            hidden = output[0]\n",
        "            steered = hidden + (coeff * vector.to(hidden.device).to(hidden.dtype))\n",
        "            return (steered,) + output[1:]\n",
        "\n",
        "        # mining subset\n",
        "        start_idx = 100\n",
        "        end_idx = len(test_data) if num_samples == -1 else 100 + num_samples\n",
        "        subset = test_data[start_idx:end_idx]\n",
        "        print(f\"Eval on {len(subset)} items...\")\n",
        "\n",
        "        cache_key = (start_idx, end_idx)\n",
        "\n",
        "        if not hasattr(self, \"baseline_cache\"):\n",
        "            self.baseline_cache = {}\n",
        "        if not hasattr(self, \"baseline_text_cache\"):\n",
        "            self.baseline_text_cache = {}\n",
        "\n",
        "        # Calculate baseline\n",
        "        if cache_key in self.baseline_cache:\n",
        "            base_success_count = self.baseline_cache[cache_key]\n",
        "            baseline_texts = self.baseline_text_cache[cache_key]\n",
        "\n",
        "            print(f\"Using cached baseline: {base_success_count}\")\n",
        "\n",
        "        else:\n",
        "            base_success_count = 0\n",
        "            baseline_texts = []\n",
        "\n",
        "            for item in tqdm(subset, desc=\"Base\"):\n",
        "                inp = self.tokenizer(item[\"prompt\"], return_tensors=\"pt\").input_ids.to(self.device)\n",
        "                out = self.model.generate(inp, max_new_tokens=20)\n",
        "                txt = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "                baseline_texts.append(txt)\n",
        "\n",
        "                if item[\"target_str\"].lower() in txt.lower():\n",
        "                    base_success_count += 1\n",
        "\n",
        "            self.baseline_cache[cache_key] = base_success_count\n",
        "            self.baseline_text_cache[cache_key] = baseline_texts\n",
        "\n",
        "        # Calculate steered\n",
        "        success = 0\n",
        "        steered_texts = []\n",
        "\n",
        "        h = self.model.decoder.block[layer_idx].register_forward_hook(hook)\n",
        "        try:\n",
        "            for item in tqdm(subset, desc=\"Steered\"):\n",
        "                inp = self.tokenizer(item[\"prompt\"], return_tensors=\"pt\").input_ids.to(self.device)\n",
        "                out = self.model.generate(inp, max_new_tokens=20)\n",
        "                txt = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "                steered_texts.append(txt)\n",
        "\n",
        "                if item[\"target_str\"].lower() in txt.lower():\n",
        "                    success += 1\n",
        "        finally:\n",
        "            h.remove()\n",
        "\n",
        "        base_acc = base_success_count / len(subset)\n",
        "        steer_acc = success / len(subset)\n",
        "\n",
        "        print()\n",
        "        print(f\"Base: {base_acc:.1%} | Steered: {steer_acc:.1%}\")\n",
        "        print(f\"Improvement:       {steer_acc - base_acc:.1%}\")\n",
        "\n",
        "        # Results\n",
        "        results = []\n",
        "        for item, base_txt, steer_txt in zip(subset, baseline_texts, steered_texts):\n",
        "            results.append({\n",
        "                \"prompt\": item[\"prompt\"],\n",
        "                \"target\": item[\"target_str\"],\n",
        "                \"baseline_gen\": base_txt,\n",
        "                \"steered_gen\": steer_txt,\n",
        "                \"baseline_correct\": item[\"target_str\"].lower() in base_txt.lower(),\n",
        "                \"steered_correct\": item[\"target_str\"].lower() in steer_txt.lower(),\n",
        "            })\n",
        "\n",
        "        return {\n",
        "            \"layer\": layer_idx,\n",
        "            \"coeff\": coeff,\n",
        "            \"base_acc\": base_acc,\n",
        "            \"steer_acc\": steer_acc,\n",
        "            \"improvement\": steer_acc - base_acc,\n",
        "            \"results\": results,\n",
        "        }\n",
        "\n",
        "def visualize_steering_result(\n",
        "    obj,\n",
        "    max_prompt_len=70,\n",
        "    max_gen_len=18,\n",
        "    only_improvements=False,\n",
        "):\n",
        "\n",
        "    def trunc(s, n):\n",
        "        if s is None:\n",
        "            return \"\"\n",
        "        return s if len(s) <= n else s[: n - 3] + \"...\"\n",
        "\n",
        "    print(\"=\" * 100)\n",
        "    print(\n",
        "        f\"Layer: {obj['layer']} | \"\n",
        "        f\"Coeff: {obj['coeff']} | \"\n",
        "        f\"Baseline Acc: {obj['base_acc']:.3f} | \"\n",
        "        f\"Steered Acc: {obj['steer_acc']:.3f} | \"\n",
        "        f\"Δ: {obj['improvement']:.3f}\"\n",
        "    )\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "    header = f\"{'B→S':<4} {'TARGET':<10} {'BASE':<18} {'STEER':<18} PROMPT\"\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "\n",
        "    shown = 0\n",
        "    for r in obj[\"results\"]:\n",
        "        base_is_ok = r.get(\"baseline_correct\", False)\n",
        "        steered_is_ok = r.get(\"steered_correct\", False)\n",
        "        \n",
        "        if only_improvements and (base_is_ok or not steered_is_ok):\n",
        "            continue\n",
        "\n",
        "        status = (\n",
        "            \"✓✓\" if base_is_ok and steered_is_ok else\n",
        "            \"✗✓\" if (not base_is_ok and steered_is_ok) else\n",
        "            \"✓✗\" if (base_is_ok and not steered_is_ok) else\n",
        "            \"✗✗\"\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"{status:<4} \"\n",
        "            f\"{r.get('target',''):<10} \"\n",
        "            f\"{trunc(r.get('baseline_gen',''), max_gen_len):<18} \"\n",
        "            f\"{trunc(r.get('steered_gen',''), max_gen_len):<18} \"\n",
        "            f\"{trunc(r.get('prompt',''), max_prompt_len)}\"\n",
        "        )\n",
        "        shown += 1\n",
        "\n",
        "    if shown == 0:\n",
        "        print(\"(no rows to display)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"GradientSteering helper class successfully loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08t2tyHyb8AV"
      },
      "outputs": [],
      "source": [
        "FILENAME = \"data/memo-trap-input-data.jsonl\"\n",
        "ACTIVATION_STRENGTH = 1000\n",
        "NUM_SAMPLES = 300\n",
        "LAYER = 5\n",
        "\n",
        "# Runs on L4 on colab\n",
        "if os.path.exists(FILENAME):\n",
        "    with open(FILENAME, \"r\") as f:\n",
        "        content = f.read()\n",
        "\n",
        "    runner = GradientSteering(model_name=\"google/flan-t5-base\")\n",
        "    data = runner.load_data(content)\n",
        "\n",
        "    print(f\"Mining gradients on Layer {LAYER}!\")\n",
        "    vec = runner.mine_gradient_vector(data, LAYER)\n",
        "    print(f\"Testing on {NUM_SAMPLES} new samples!\")\n",
        "    results = runner.test(data, vec, LAYER, ACTIVATION_STRENGTH, NUM_SAMPLES)\n",
        "else:\n",
        "    print(f\"File not found: {FILENAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM45zhWoaoia"
      },
      "outputs": [],
      "source": [
        "# Can change only_improvements to False if you want to see all\n",
        "visualize_steering_result(results, only_improvements=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
